# Policy Paper Theme & Cluster Analysis (Martens Centre)

This project conducts a comprehensive text analysis of policy papers published by the Wilfried Martens Centre for European Studies. Using a combination of Natural Language Processing (NLP), semantic embeddings, clustering, and keyword extraction, the notebook identifies thematic structures and content patterns across a body of policy research.

---

## Objectives

- **Clean** and preprocess raw full-text PDF documents.
- **Detect dominant themes** in each paper using sentence embeddings.
- **Cluster** themes into higher-level topic areas.
- **Visualize** theme co-occurrence, distribution by cluster, and representative keywords.
- **Export** structured CSV summaries for further reporting or visual use.

---

## Scripts Overview

The following Python scripts are provided in the `scripts/` directory to replicate the data acquisition process:

- `download_martens_pdfs.py`: Downloads policy paper PDFs from Martens Centre.
- `download_multi_pdfs.py`: Alternate downloader for a list of PDF links.
- `extract.texts.py`: Extracts full text from PDFs and saves it to `martens_texts.csv`.
- `scrape_summaries.py`: Scrapes metadata and summaries from article pages using BeautifulSoup. These outputs were used for manual reference and exploratory analysis but are not integrated into the final pipeline.

These scripts allow you to regenerate the dataset if the structure of `martens_texts.csv` is lost or if new papers are added in the future.

---

## Input Data

- `martens_texts.csv` — Full-text of Martens Centre policy papers (preprocessed)

---

## Tools & Methodology

| Tools                         | Description |
|------------------------------------|-------------|
| `SentenceTransformers`             | Generates semantic vector embeddings for article texts and predefined themes |
| `cosine_similarity`                | Measures semantic similarity between article embeddings and theme embeddings |
| `AgglomerativeClustering` (`ward` + `euclidean`) | Groups similar articles based on embedding distances into interpretable macro-clusters |
| `matplotlib`, `seaborn`            | Produces visualisations (dendrograms, heatmaps, bar charts) for thematic and cluster analysis |
| `wordcloud`                        | Generates a visual summary of frequently occurring key terms |
| `BeautifulSoup`, `requests`       | Web scraping of article metadata and summaries from policy portals |
| `pandas`, `numpy`                  | Data wrangling, summary statistics, and cluster assignment logic |


---

## Outputs

The following outputs are generated by the scripts and notebook:

**Tables (`data/`)**
- `martens_texts.csv`: Full text dataset with metadata.
- `wmc_summaries.csv`: Scraped article summaries.
- `links.csv`: URLs and metadata for source PDFs.
- `text_descriptives_summary.csv`: Corpus-level statistics (e.g. word count, sentence count).
- `cluster.csv`: Cluster assignment for each document.
- `theme_counts_table.csv`: Raw count of theme keywords across articles.
- `theme_summary_table.csv`: Aggregated metrics for each identified theme.
- `top_words_frequency.csv`: Most frequent words across all articles.
- `top_20_words_per_article.csv`: Top 20 words per document.
- `top_representative_papers.csv`: Top-ranked article(s) per cluster based on cosine similarity.

**Visuals (`images/`)**
- `hierarchical_clustering_dendrogram.png`: Document similarity visualized as a dendrogram.
- `theme_cooccurrence_heatmap.png`: Heatmap of co-occurring themes across documents.
- `theme_frequency_heatmap.png`: Raw frequency of each theme.
- `theme_frequency_within_clusters.png`: Theme frequency broken down by cluster.
- `wordcloud.png`: Word cloud of top keywords in the corpus.

---

## Theming

Themes were grouped into 5 higher-level clusters:

1. **Society, Politics & Information Order**
2. **Geopolitics & Strategic Regions**
3. **Economy, Energy & Innovation**
4. **Social Europe & Labour Dynamics**
5. **Defence & Transatlantic Security**

Each theme was mapped to keywords and matched via semantic similarity to papers.

---

## How to Use

1. Clone this repository
2. Open `Analysis.ipynb` in JupyterLab or VSCode
3. Run cells top to bottom — intermediate `.csv` and `.png` files are saved automatically

---

## Author

**Edvardas Lukošius**  
Vilnius, Lithuania  

